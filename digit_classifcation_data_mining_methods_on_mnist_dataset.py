# -*- coding: utf-8 -*-
"""Digit Classifcation - Data Mining Methods on MNIST Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sT5e6-8IJgB_wYDuFUOtiGfGlTP5xEXj
"""

!pip install bayesian-optimization

import numpy as np
import pandas as pd
from keras.datasets import mnist
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, classification_report,confusion_matrix

"""#IMPORTING DATASET"""

(X_train, y_train), (X_test, y_test) = mnist.load_data()

from matplotlib import pyplot
for i in range(3):
  pyplot.subplot(330 + 1 + i)
  pyplot.imshow(X_train[i], cmap=pyplot.get_cmap('gray'))
  pyplot.show()

X_train = X_train.reshape(-1, 28*28)
X_test = X_test.reshape(-1, 28*28)

"""Normalizing the pixel values to range [0, 1]"""

X_train = X_train / 255.0
X_test = X_test / 255.0

"""#IMPLEMENTING TRAINING MODELS

DECISION TREE CLASSIFIER
"""

dt_classifier = DecisionTreeClassifier(random_state=42)
dt_scores = cross_val_score(dt_classifier, X_train, y_train, cv=5)
print("Decision Tree Classifier Accuracy: {:.2f}%".format(dt_scores.mean() * 100))

"""RANDOM FOREST CLASSIFIER"""

rf_classifier = RandomForestClassifier(random_state=42)
rf_scores = cross_val_score(rf_classifier, X_train, y_train, cv=5)
print("Random Forest Classifier Accuracy: {:.2f}%".format(rf_scores.mean() * 100))

"""NAIVE BAYES CLASSIFIER"""

nb_classifier = GaussianNB()
nb_scores = cross_val_score(nb_classifier, X_train, y_train, cv=5)
print("Naïve Bayes Classifier Accuracy: {:.2f}%".format(nb_scores.mean() * 100))

"""KNN CLASSIFIER"""

knn_classifier = KNeighborsClassifier()
knn_scores = cross_val_score(knn_classifier, X_train, y_train, cv=5)
print("KNN Classifier Accuracy: {:.2f}%".format(knn_scores.mean() * 100))

"""NEURAL NETWORK CLASSIFIER"""

nn_classifier = MLPClassifier(random_state=42)
nn_scores = cross_val_score(nn_classifier, X_train, y_train, cv=5)
print("Neural Network Classifier Accuracy: {:.2f}%".format(nn_scores.mean() * 100))

"""Comparing through Graph

"""

import matplotlib.pyplot as plt
import seaborn as sns

sns.set(style='whitegrid')
palette = sns.color_palette('viridis')

classifiers = ['Decision Tree', 'Random Forest', 'Naive Bayes', 'K-Nearest Neighbors', 'Multilayer Perceptron']
# classifier_accuracies=[86.65, 96.64, 56.18, 96.93, 97.47]
classifier_accuracies=[dt_scores.mean(), rf_scores.mean(), nb_scores.mean(), knn_scores.mean(), nn_scores.mean()]

plt.figure(figsize=(10, 6))
bars = plt.bar(classifiers, classifier_accuracies, color=palette, width=0.5)

plt.xlabel('Classifier', fontsize=12, fontweight='bold')
plt.ylabel('Accuracy', fontsize=12, fontweight='bold')
plt.title('Comparison of Classifier Accuracies on MNIST Dataset', fontsize=14, fontweight='bold')

plt.grid(axis='y', linestyle='--', alpha=0.6)

for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width() / 2, yval + 0.01, f'{yval:.2f}', ha='center', va='bottom', fontsize=10, fontweight='bold')

plt.show()

data = {'Classifier': classifiers, 'Accuracy': classifier_accuracies}
df = pd.DataFrame(data)
df_sorted = df.sort_values(by='Accuracy', ascending=False)
df_sorted = df_sorted.reset_index(drop=True)
df_sorted['Rank'] = df_sorted.index + 1
print(df_sorted)

best_classifier = df_sorted.iloc[0]['Classifier']
best_accuracy = df_sorted.iloc[0]['Accuracy']
print(f"\nThe classifier with the highest accuracy ({best_accuracy:.4f}) is {best_classifier}, making it the best for classification.")

"""#EVALUATION METRICS"""

classifiers = [dt_classifier, rf_classifier, nb_classifier, knn_classifier, nn_classifier]
classifier_names = ["Decision Tree", "Random Forest", "Naïve Bayes", "KNN", "Neural Network"]

for clf, name in zip(classifiers, classifier_names):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred)
    conf_matrix = confusion_matrix(y_test, y_pred)
    print(f"\n{name} Evaluation Metrics:")
    print(f"Accuracy: {accuracy}")
    print(f"Classification Report:\n{report}")
    print(f"Confusion matrix:\n{conf_matrix}")
    print('-'*150)

"""#PARAMETER TUNING

## Through Grid search

Defining parameter grid for each classifier
"""

dt_classifier = DecisionTreeClassifier(random_state=42)
rf_classifier = RandomForestClassifier(random_state=42)
knn_classifier = KNeighborsClassifier()
nn_classifier = MLPClassifier(random_state=42)

param_grid_dt = {
     'criterion': ['gini', 'entropy'],
     'splitter': ['best', 'random'],
    'max_depth': [10, 20, 30,],
    'min_samples_split': [2, 5,],
     'min_samples_leaf': [1, 2, 4]
}

param_grid_rf = {
    'n_estimators': [50, 100],
     'criterion': ['gini', 'entropy'],
    'max_depth': [10, 20, 30],
     'min_samples_split': [2, 5, 10],
     'min_samples_leaf': [1, 2, 4]
}

param_grid_knn = {
    'n_neighbors': [3, 5, 7],
     'weights': ['uniform', 'distance'],
     'metric': ['euclidean', 'manhattan']
}

param_grid_nn = {
    'hidden_layer_sizes': [(50,), (100,)],
    'activation': ['logistic', 'relu'],
     'solver': ['adam', 'sgd'],
     'alpha': [0.0001, 0.001, 0.01]
}

"""Grid search for each classifier"""

grid_search_dt = GridSearchCV(dt_classifier, param_grid_dt, cv=5)
grid_search_rf = GridSearchCV(rf_classifier, param_grid_rf, cv=5)
grid_search_knn = GridSearchCV(knn_classifier, param_grid_knn, cv=5)
grid_search_nn = GridSearchCV(nn_classifier, param_grid_nn, cv=5)

"""Fit the grid search objects"""

grid_search_dt.fit(X_train, y_train)
grid_search_rf.fit(X_train, y_train)
grid_search_knn.fit(X_train, y_train)
grid_search_nn.fit(X_train, y_train)

"""Best parameters for each classifier"""

print("Best parameters for Decision Tree:", grid_search_dt.best_params_)
print("Best parameters for Random Forest:", grid_search_rf.best_params_)
print("Best parameters for KNN:", grid_search_knn.best_params_)
print("Best parameters for Neural Network:", grid_search_nn.best_params_)

"""##Through Parameter Search

###Random Forest

Random Search
"""

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

# import warnings filter
from warnings import simplefilter
# ignore all future warnings
simplefilter(action='ignore', category=FutureWarning)


param_dist = {
    'n_estimators': randint(10, 200),
    'max_depth': randint(1, 20),
    'min_samples_split': randint(2, 10),
    'min_samples_leaf': randint(1, 10),
    'max_features': ['auto', 'sqrt', 'log2']
}

rf_classifier = RandomForestClassifier(random_state=42)

random_search = RandomizedSearchCV(rf_classifier, param_distributions=param_dist, n_iter=20, cv=5, random_state=42)

random_search.fit(X_train, y_train);


print(f"Best parameters: {random_search.best_params_}")
print(f"Best cross-validation score: {random_search.best_score_:.4f}")

"""Bayesian Optimization"""

from bayes_opt import BayesianOptimization

def rf_cv(n_estimators, max_depth, min_samples_split, min_samples_leaf):
    rf = RandomForestClassifier(
        n_estimators=int(n_estimators),
        max_depth=int(max_depth),
        min_samples_split=int(min_samples_split),
        min_samples_leaf=int(min_samples_leaf),
        random_state=42
    )
    cv_scores = cross_val_score(rf, X_train, y_train, cv=5)
    return cv_scores.mean()

pbounds = {
    'n_estimators': (10, 200),
    'max_depth': (1, 20),
    'min_samples_split': (2, 10),
    'min_samples_leaf': (1, 10),
}

# Initialize the Bayesian optimizer
optimizer = BayesianOptimization(
    f=rf_cv,
    pbounds=pbounds,
    random_state=42,
)

# Optimize
optimizer.maximize(init_points=5, n_iter=15)

# Print the best parameters and the corresponding best score
print(f"Best parameters: {optimizer.max['params']}")
print(f"Best cross-validation score: {optimizer.max['target']:.4f}")

"""###Decision Tree

Random Search
"""

param_dist_dt = {
'max_depth': randint(1, 20),
'min_samples_split': randint(2, 10),
'min_samples_leaf': randint(1, 10)
}

dt_classifier = DecisionTreeClassifier(random_state=42)

random_search_dt = RandomizedSearchCV(
dt_classifier, param_distributions=param_dist_dt, n_iter=1, cv=5, random_state=42
)
random_search_dt.fit(X_train, y_train)

print(f"Decision Tree Random Search - Best Parameters: {random_search_dt.best_params_}")
print(f"Decision Tree Random Search - Best CV Score: {random_search_dt.best_score_:.4f}")

"""Bayesian Optimization"""

# Define function to optimize
def dt_cv(max_depth, min_samples_split, min_samples_leaf):
    dt = DecisionTreeClassifier(
        max_depth=int(max_depth),
        min_samples_split=int(min_samples_split),
        min_samples_leaf=int(min_samples_leaf),
        random_state=42
    )
    cv_scores = cross_val_score(dt, X_train, y_train, cv=5)
    return cv_scores.mean()

# Define parameter bounds for Bayesian optimization
pbounds_dt = {
    'max_depth': (1, 20),
    'min_samples_split': (2, 10),
    'min_samples_leaf': (1, 10)
}

# Initialize Bayesian optimizer
optimizer_dt = BayesianOptimization(
    f=dt_cv,
    pbounds=pbounds_dt,
    random_state=42
)

# Optimize
optimizer_dt.maximize(init_points=5, n_iter=15)

# Print the best parameters and score
print(f"Decision Tree Bayesian Optimization - Best Parameters: {optimizer_dt.max['params']}")
print(f"Decision Tree Bayesian Optimization - Best CV Score: {optimizer_dt.max['target']:.4f}")

"""###KNN

Random Search
"""

# Define parameter distributions for KNN
param_dist_knn = {
    'n_neighbors': randint(1, 20),
    'weights': ['uniform', 'distance'],
    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
    'leaf_size': randint(10, 50),
    'p': randint(1, 2)
}

# Initialize K-Nearest Neighbors Classifier
knn_classifier = KNeighborsClassifier()

# Perform Random Search
random_search_knn = RandomizedSearchCV(
    knn_classifier, param_distributions=param_dist_knn, n_iter=20, cv=5, random_state=42
)
random_search_knn.fit(X_train, y_train)

# Print the best parameters and score
print(f"K-Nearest Neighbors Random Search - Best Parameters: {random_search_knn.best_params_}")
print(f"K-Nearest Neighbors Random Search - Best CV Score: {random_search_knn.best_score_:.4f}")

"""Bayesian Optimization"""

# Define function to optimize
def knn_cv(n_neighbors, leaf_size, p):
    knn = KNeighborsClassifier(
        n_neighbors=int(n_neighbors),
        leaf_size=int(leaf_size),
        p=int(p),
        weights='uniform',
        algorithm='auto'
    )
    cv_scores = cross_val_score(knn, X_train, y_train, cv=5)
    return cv_scores.mean()

# Define parameter bounds for Bayesian optimization
pbounds_knn = {
    'n_neighbors': (1, 20),
    'leaf_size': (10, 50),
    'p': (1, 2)
}

# Initialize Bayesian optimizer
optimizer_knn = BayesianOptimization(
    f=knn_cv,
    pbounds=pbounds_knn,
    random_state=42
)

# Optimize
optimizer_knn.maximize(init_points=5, n_iter=15)

# Print the best parameters and score
print(f"K-Nearest Neighbors Bayesian Optimization - Best Parameters: {optimizer_knn.max['params']}")
print(f"K-Nearest Neighbors Bayesian Optimization - Best CV Score: {optimizer_knn.max['target']:.4f}")

"""###Neural Networks

Random Search
"""

# Define parameter distributions for MLP
param_dist_mlp = {
    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100)],
    'activation': ['relu', 'tanh', 'logistic'],
    'solver': ['adam', 'sgd'],
    'alpha': uniform(0.0001, 0.1),
    'learning_rate': ['constant', 'invscaling', 'adaptive'],
    'learning_rate_init': uniform(0.001, 0.1),
    'max_iter': randint(100, 1000)
}

# Initialize MLPClassifier
mlp_classifier = MLPClassifier(random_state=42)

# Perform Random Search
random_search_mlp = RandomizedSearchCV(
    mlp_classifier, param_distributions=param_dist_mlp, n_iter=20, cv=5, random_state=42
)
random_search_mlp.fit(X_train, y_train)

# Print the best parameters and score
print(f"MLP Random Search - Best Parameters: {random_search_mlp.best_params_}")
print(f"MLP Random Search - Best CV Score: {random_search_mlp.best_score_:.4f}")

"""Bayesian Optimization"""

# Define function to optimize
def mlp_cv(hidden_layer_sizes, activation, solver, alpha, learning_rate, learning_rate_init, max_iter):
    mlp = MLPClassifier(
        hidden_layer_sizes=eval(hidden_layer_sizes),
        activation=activation,
        solver=solver,
        alpha=alpha,
        learning_rate=learning_rate,
        learning_rate_init=learning_rate_init,
        max_iter=max_iter,
        random_state=42
    )
    cv_scores = cross_val_score(mlp, X_train, y_train, cv=5)
    return cv_scores.mean()

# Define parameter bounds for Bayesian optimization
pbounds_mlp = {
    'hidden_layer_sizes': ['(50,)', '(100,)', '(50,50)', '(100,100)'],
    'activation': ['relu', 'tanh', 'logistic'],
    'solver': ['adam', 'sgd'],
    'alpha': (0.0001, 0.1),
    'learning_rate': ['constant', 'invscaling', 'adaptive'],
    'learning_rate_init': (0.001, 0.1),
    'max_iter': (100, 1000)
}

# Initialize Bayesian optimizer
optimizer_mlp = BayesianOptimization(
    f=mlp_cv,
    pbounds=pbounds_mlp,
    random_state=42
)

# Optimize
optimizer_mlp.maximize(init_points=5, n_iter=15)

# Print the best parameters and score
print(f"MLP Bayesian Optimization - Best Parameters: {optimizer_mlp.max['params']}")
print(f"MLP Bayesian Optimization - Best CV Score: {optimizer_mlp.max['target']:.4f}")